{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Model for Measuring Sentence Relatedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd \n",
    "\n",
    "######\n",
    "# TODO: Download dataset: https://github.com/Priya22/semantic-textual-relatedness/blob/master/sem_text_rel_ranked.csv\n",
    "#       Store CSV file in data directory\n",
    "\n",
    "df = pd.read_csv('data/sem_text_rel_ranked.csv')\n",
    "\n",
    "train_examples = df.apply(lambda x: InputExample(texts=x['Text'].split('\\n'), label=x['Score']), axis=1)\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "model = SentenceTransformer('bert-base-cased')\n",
    "\n",
    "train_examples = df.apply(lambda x: InputExample(texts=x['Text'].split('\\n'), label=x['Score']), axis=1)\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n",
    "\n",
    "model.save(\"models/sbert-relatedness\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Relatedness of Endpoints in Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ROCStories\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def get_similarity(a, b):\n",
    "    n = np.dot(a, b)\n",
    "    a_norm = np.sqrt(np.sum(a**2))\n",
    "    b_norm = np.sqrt(np.sum(b**2))\n",
    "    d = a_norm * b_norm\n",
    "    return n/d\n",
    "\n",
    "model = SentenceTransformer('models/sbert-relatedness')\n",
    "\n",
    "#####\n",
    "# TODO: Complete form to download dataset: https://cs.rochester.edu/nlp/rocstories/\n",
    "#       Store all CSV files in data directory\n",
    "\n",
    "df16 = pd.read_csv('data/ROCStories__spring2016 - ROCStories_spring2016.csv')\n",
    "df17 = pd.read_csv('data/ROCStories_winter2017 - ROCStories_winter2017.csv')\n",
    "\n",
    "df = pd.concat([df16, df17], axis=0)    # combine both datasets\n",
    "endpoints = df.loc[:,'sentence1':'sentence5']\n",
    "\n",
    "start = endpoints[\"sentence1\"].values.tolist()\n",
    "stop = endpoints[\"sentence5\"].values.tolist()\n",
    "\n",
    "start_embeddings = model.encode(start)\n",
    "stop_embeddings = model.encode(stop)\n",
    "\n",
    "scores = [get_similarity(start, stop) for start,stop in zip(start_embeddings, stop_embeddings)]\n",
    "\n",
    "df['score'] = scores\n",
    "df = df.sort_values(by='score', ascending=True).reset_index(drop=True)\n",
    "df.to_csv('data/roc_scored.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of sub-datasets for each score (higher score indicates endpoints are more related)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/roc_scored.csv')\n",
    "\n",
    "score_min = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\n",
    "for m in score_min:\n",
    "  story_count = sum(df['score'] >= m)\n",
    "  print(\"# stories scoring >= {}:  {}\".format(m, story_count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune GPT Baseline 3x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "  \n",
    "df = pd.read_csv('data/roc_scored.csv', usecols=['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'score'])\n",
    "df['rocstory'] = df.sentence1 + ' ' + df.sentence2 + ' ' + df.sentence3 + ' ' + df.sentence4 + ' ' + df.sentence5 + '<|endoftext|>'\n",
    "\n",
    "df = df.rename(columns={'sentence1':'start'})\n",
    "df = df.drop(columns=['sentence2', 'sentence3', 'sentence4', 'sentence5'])\n",
    "df = df.sample(frac=1, random_state=104).reset_index(drop=True)\n",
    "\n",
    "# Sanity check\n",
    "print(df.at[5, 'rocstory'])\n",
    "\n",
    "\n",
    "# Build datasets \n",
    "# Originally train_test_ratio = 0.9; train_valid_ratio = 7/9\n",
    "# Now using story cloze corpus for testing!\n",
    "\n",
    "train_valid_ratio = 0.8     # 80% training; 20% validation \n",
    "df_train, df_valid = train_test_split(df, train_size=train_valid_ratio, random_state=1)\n",
    "\n",
    "def build_dataset_csv(df, dest_path):\n",
    "    df.to_csv(dest_path+'.csv', index=False)\n",
    "\n",
    "def build_dataset_txt(df, dest_path):\n",
    "    f = open(dest_path+'.txt', 'w')\n",
    "    stories = df['rocstory'].tolist()\n",
    "    for s in stories:\n",
    "        s = str(s).strip()\n",
    "        s = re.sub(r\"\\s\", \" \", s)\n",
    "        f.write(s + '\\n')\n",
    "\n",
    "rounds = ['F1', 'F2', 'F3']\n",
    "\n",
    "for r in rounds:\n",
    "    if r == 'F1':\n",
    "        score = 0.0\n",
    "    if r == 'F2':\n",
    "        score = 0.3\n",
    "    if r == 'F3':\n",
    "        score = 0.5\n",
    "\n",
    "    # Create new directory\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    new_dirs_path = os.path.join('data', f'baseline_{r}')\n",
    "    os.makedirs(new_dirs_path, exist_ok=True)\n",
    "\n",
    "    roc = df[df['score'] >= score].reset_index(drop=True)\n",
    "    df_train, df_valid = train_test_split(roc, train_size=train_valid_ratio, random_state=1)\n",
    "\n",
    "    build_dataset_csv(df_train, f'data/baseline_{r}/train')\n",
    "    build_dataset_csv(df_valid, f'data/baseline_{r}/valid')\n",
    "    build_dataset_txt(df_train, f'data/baseline_{r}/train')\n",
    "    build_dataset_txt(df_valid, f'data/baseline_{r}/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# note: test data is joint cloze2016 testing and validation datasets\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file            data/baseline_F1/train.txt \\\n",
    "        --validation_file       data/baseline_F1/valid.txt \\\n",
    "        --test_file             data/baseline_F1/valid.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path    gpt2 \\\n",
    "        --output_dir            models/baseline \\\n",
    "        --gradient_accumulation_steps 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x \n",
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file            data/baseline_F2/train.txt \\\n",
    "        --validation_file       data/baseline_F2/valid.txt \\\n",
    "        --test_file             data/baseline_F2/valid.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path    models/baseline \\\n",
    "        --output_dir            models/baseline_F2 \\\n",
    "        --gradient_accumulation_steps 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3X\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file            data/baseline_F3/train.txt \\\n",
    "        --validation_file       data/baseline_F3/valid.txt \\\n",
    "        --test_file             data/baseline_F3/valid.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path    models/baseline_F2 \\\n",
    "        --output_dir            models/baseline_F3 \\\n",
    "        --gradient_accumulation_steps 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Stop Generators\n",
    "\n",
    "### Stop Baseline: Given start, generate stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "rocs = pd.read_csv('data/roc_scored.csv', usecols=[\"sentence1\", \"sentence5\", \"score\"])\n",
    "\n",
    "def build_dataset(df, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    stories = df['target'].tolist()\n",
    "    for s in stories:\n",
    "        s = str(s).strip()\n",
    "        s = re.sub(r\"\\s\", \" \", s)\n",
    "        # bos_token = '<BOS>'\n",
    "        eos_token = '<|endoftext|>'\n",
    "        data += s + eos_token + '\\n'\n",
    "    f.write(data)\n",
    "\n",
    "\n",
    "# Set up for 3-way fine-tuning\n",
    "rounds = ['F1', 'F2', 'F3']\n",
    "for r in rounds:\n",
    "    if r == 'F1':\n",
    "        score = 0.0\n",
    "    if r == 'F2':\n",
    "        score = 0.3\n",
    "    if r == 'F3':\n",
    "        score = 0.7\n",
    "\n",
    "    rocs_subset = rocs[rocs['score'] >= score].reset_index(drop=True)\n",
    "    rocs_subset['target'] = rocs_subset['sentence1'] + ' ' + rocs_subset['sentence5']\n",
    "    rocs_subset.rename(columns = {'sentence1':'start', 'sentence5':'stop'}, inplace = True)\n",
    "\n",
    "    # Build datasets\n",
    "    train_test_ratio = 0.9\n",
    "    train_valid_ratio = 7/9\n",
    "\n",
    "    df_full_train, df_test = train_test_split(rocs_subset, train_size = train_test_ratio, random_state = 1)\n",
    "    df_train, df_valid = train_test_split(df_full_train, train_size = train_valid_ratio, random_state = 1)\n",
    "\n",
    "    os.makedirs('data', exist_ok=True)                              # check 'data' directory exists\n",
    "    new_dirs_path = os.path.join('data', f'stop_baseline_{r}')      # define new directory within\n",
    "    os.makedirs(new_dirs_path, exist_ok=True)                       # create new directory\n",
    "\n",
    "    # df_train.to_csv(f'data/stop_baseline_{r}/train.csv', index=False)\n",
    "    # df_valid.to_csv(f'data/stop_baseline_{r}/valid.csv', index=False)\n",
    "    # df_test.to_csv(f'data/stop_baseline_{r}/test.csv', index=False)\n",
    "\n",
    "    build_dataset(df_train, f'data/stop_baseline_{r}/train.txt')\n",
    "    build_dataset(df_valid, f'data/stop_baseline_{r}/valid.txt')\n",
    "    build_dataset(df_test, f'data/stop_baseline_{r}/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# TODO: download run_clm_no_trainer.py from https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "#       place in renargen_lm directory\n",
    "# Train F1\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file            data/stop_baseline_F1/train.txt \\\n",
    "        --validation_file       data/stop_baseline_F1/valid.txt \\\n",
    "        --test_file             data/stop_baseline_F1/test.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path    models/baseline \\\n",
    "        --output_dir            models/stop_baseline_F1 \\\n",
    "        --gradient_accumulation_steps 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train F2\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file            data/stop_baseline_F2/train.txt \\\n",
    "        --validation_file       data/stop_baseline_F2/valid.txt \\\n",
    "        --test_file             data/stop_baseline_F2/test.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path    models/stop_baseline_F1 \\\n",
    "        --output_dir            models/stop_baseline_F2 \\\n",
    "        --gradient_accumulation_steps 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train F3\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file            data/stop_baseline_F3/train.txt \\\n",
    "        --validation_file       data/stop_baseline_F3/valid.txt \\\n",
    "        --test_file             data/stop_baseline_F3/test.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path    models/stop_baseline_F2 \\\n",
    "        --output_dir            models/stop_baseline_F3 \\\n",
    "        --gradient_accumulation_steps 8 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Generator: Given start, generate list of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('data/roc_scored.csv', usecols=['sentence1', 'sentence5', 'score'])\n",
    "\n",
    "df = df.loc[:, ['sentence1','sentence5']].applymap(word_tokenize)\n",
    "\n",
    "# Find overlap, maintain order of stop sentence\n",
    "get_overlap = lambda x, y: [i for i in x if i in y]       \n",
    "df['rep'] = df.apply(lambda x: list(get_overlap(x['sentence1'], x['sentence5'])), axis=1)\n",
    "df['rep'] = df['rep'].apply(lambda x: \", \".join(x)).apply(lambda x: '[' + x + ']')\n",
    "\n",
    "df_o = pd.read_csv('data/roc_scored.csv', usecols=['sentence1', 'sentence5', 'score'])\n",
    "\n",
    "df['sentence1'] = df_o['sentence1']\n",
    "df['sentence5'] = df_o['sentence5']\n",
    "df['score'] = df_o['score']\n",
    "df.rename(columns = {'sentence1':'start', 'sentence5':'stop'}, inplace = True)\n",
    "\n",
    "df['target'] = df['start'] + ' ' + df['stop'] + ' ' + df['rep'] + '<|endoftext|>'\n",
    "\n",
    "# Sanity check\n",
    "print(df.at[90000,'target'])\n",
    "\n",
    "def build_dataset(df, dest_path):\n",
    "    f = open(dest_path+'.txt', 'w')\n",
    "    data = ''\n",
    "    stories = df.target.tolist()\n",
    "    for s in stories:\n",
    "        s = str(s).strip()\n",
    "        s = re.sub(r\"\\s\", \" \", s)\n",
    "        data += s + '\\n'\n",
    "    f.write(data)\n",
    "    df.to_csv(dest_path+'.csv', index=False)\n",
    "\n",
    "# Build datasets\n",
    "train_test_ratio = 0.9\n",
    "train_valid_ratio = 7/9\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, train_size = train_test_ratio, random_state = 1)\n",
    "df_train, df_valid = train_test_split(df_full_train, train_size = train_valid_ratio, random_state = 1)\n",
    "\n",
    "os.makedirs('data', exist_ok=True)              \n",
    "new_dirs_path = os.path.join('data', 'phrase_generator') \n",
    "os.makedirs(new_dirs_path, exist_ok=True)          \n",
    "\n",
    "build_dataset(df_train, 'data/phrase_generator/train')\n",
    "build_dataset(df_valid, 'data/phrase_generator/valid')\n",
    "build_dataset(df_test, 'data/phrase_generator/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "                --train_file        data/phrase_generator/train.txt \\\n",
    "                --validation_file   data/phrase_generator/valid.txt \\\n",
    "                --test_file         data/phrase_generator/test.txt \\\n",
    "                --do_train \\\n",
    "                --model_name_or_path models/stop_baseline_F3 \\\n",
    "                --output_dir         models/phrase_generator \\\n",
    "                --gradient_accumulation_steps 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Generator: Given start + rep phrases, generate stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('data/roc_scored.csv', usecols=['sentence1', 'sentence5', 'score'])\n",
    "\n",
    "df = df.loc[:, ['sentence1','sentence5']].applymap(word_tokenize)\n",
    "\n",
    "# Find overlap, maintain order of stop sentence\n",
    "get_overlap = lambda x, y: [i for i in x if i in y]       \n",
    "df['rep'] = df.apply(lambda x: list(get_overlap(x['sentence1'], x['sentence5'])), axis=1)\n",
    "df['rep'] = df['rep'].apply(lambda x: \", \".join(x)).apply(lambda x: '[' + x + ']')\n",
    "\n",
    "df_o = pd.read_csv('data/roc_scored.csv', usecols=['sentence1', 'sentence5', 'score'])\n",
    "\n",
    "df['sentence1'] = df_o['sentence1']\n",
    "df['sentence5'] = df_o['sentence5']\n",
    "df['score'] = df_o['score']\n",
    "df.rename(columns = {'sentence1':'start', 'sentence5':'stop'}, inplace = True)\n",
    "\n",
    "df['target'] = df['start'] + df['rep'] + ' ' + df['stop'] + ' ' + '<|endoftext|>'\n",
    "\n",
    "# Sanity check\n",
    "print(df.at[90000,'target'])\n",
    "\n",
    "def build_dataset(df, dest_path):\n",
    "    f = open(dest_path+'.txt', 'w')\n",
    "    data = ''\n",
    "    stories = df.target.tolist()\n",
    "    for s in stories:\n",
    "        s = str(s).strip()\n",
    "        s = re.sub(r\"\\s\", \" \", s)\n",
    "        data += s + '\\n'\n",
    "    f.write(data)\n",
    "    df.to_csv(dest_path+'.csv', index=False)\n",
    "\n",
    "# Build datasets\n",
    "train_test_ratio = 0.9\n",
    "train_valid_ratio = 7/9\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, train_size = train_test_ratio, random_state = 1)\n",
    "df_train, df_valid = train_test_split(df_full_train, train_size = train_valid_ratio, random_state = 1)\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "new_dirs_path = os.path.join('data', 'stop_generator') \n",
    "os.makedirs(new_dirs_path, exist_ok=True)              \n",
    "\n",
    "build_dataset(df_train, 'data/stop_generator/train')\n",
    "build_dataset(df_valid, 'data/stop_generator/valid')\n",
    "build_dataset(df_test, 'data/stop_generator/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "                --train_file        data/stop_generator/train.txt \\\n",
    "                --validation_file   data/stop_generator/valid.txt \\\n",
    "                --test_file         data/stop_generator/test.txt \\\n",
    "                --do_train \\\n",
    "                --model_name_or_path models/stop_baseline_F3 \\\n",
    "                --output_dir         models/stop_generator \\\n",
    "                --gradient_accumulation_steps 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Story Infillers\n",
    "### Position Classifier: Determine where to infill next sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "rocs_path = 'data/roc_scored.csv'\n",
    "rocs = pd.read_csv(rocs_path, usecols=['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5'])\n",
    "\n",
    "\n",
    "### Generate TRUE samples\n",
    "\n",
    "true_sample_list = []\n",
    "for i in range(50000):\n",
    "\n",
    "    # Choose middle sentence\n",
    "    index_m = random.randint(2,4)\n",
    "    # print(index_m)\n",
    "\n",
    "    # Choose LC and RC\n",
    "    potentials_lc = [j for j in range(1,index_m)]\n",
    "    potentials_rc = [j for j in range(index_m+1, 6)]\n",
    "    \n",
    "    index_lc = sorted(random.sample(potentials_lc, random.choice(potentials_lc)))\n",
    "    index_rc = sorted(random.sample(potentials_rc, random.choice(potentials_rc)-index_m))\n",
    "\n",
    "    # Sample story from ROCStories dataset\n",
    "    index_story = random.randint(0, 98160)\n",
    "    lc = \"\"\n",
    "    rc = \"\" \n",
    "\n",
    "    if 1 in index_lc:\n",
    "        lc += rocs.at[index_story, 'sentence1'] + \" \"\n",
    "    if 2 in index_lc:\n",
    "        lc += rocs.at[index_story, 'sentence2'] + \" \"\n",
    "    if 3 in index_lc:\n",
    "        lc += rocs.at[index_story, 'sentence3'] + \" \"\n",
    "\n",
    "    if 3 in index_rc:\n",
    "        rc +=  \" \" + rocs.at[index_story, 'sentence3']\n",
    "    if 4 in index_rc:\n",
    "        rc += \" \" + rocs.at[index_story, 'sentence4']\n",
    "    if 5 in index_rc:\n",
    "        rc += \" \" + rocs.at[index_story, 'sentence5']\n",
    "\n",
    "    true_sample_list.append(lc + \"<mask>\" + rc)\n",
    "\n",
    "\n",
    "### Generate FALSE samples\n",
    "\n",
    "false_sample_list = []\n",
    "\n",
    "for i in range(50000):\n",
    "\n",
    "    # Choose middle sentence\n",
    "    index_m = random.randint(2,4)\n",
    "    # print(index_m)\n",
    "\n",
    "    # Choose LC and RC\n",
    "    potentials_lc = [j for j in range(1,index_m)]\n",
    "    potentials_rc = [j for j in range(index_m+1, 6)]\n",
    "    \n",
    "    index_lc = sorted(random.sample(potentials_lc, random.choice(potentials_lc)))\n",
    "    index_rc = sorted(random.sample(potentials_rc, random.choice(potentials_rc)-index_m))\n",
    "\n",
    "    # Only choose lc+rc < 5 \n",
    "    while len(index_lc) + len(index_rc) == 4:       # 4 since m is appended below\n",
    "        index_lc = sorted(random.sample(potentials_lc, random.choice(potentials_lc)))\n",
    "        index_rc = sorted(random.sample(potentials_rc, random.choice(potentials_rc)-index_m))\n",
    "\n",
    "    # Sample story from ROCStories dataset\n",
    "    index_story = random.randint(0, 98160)\n",
    "    lc = \"\"\n",
    "    rc = \"\" \n",
    "\n",
    "    if 1 in index_lc:\n",
    "        lc += rocs.at[index_story, 'sentence1'] + \" \"\n",
    "    if 2 in index_lc:\n",
    "        lc += rocs.at[index_story, 'sentence2'] + \" \"\n",
    "    if 3 in index_lc:\n",
    "        lc += rocs.at[index_story, 'sentence3'] + \" \"\n",
    "\n",
    "    if 3 in index_rc:\n",
    "        rc +=  \" \" + rocs.at[index_story, 'sentence3']\n",
    "    if 4 in index_rc:\n",
    "        rc += \" \" + rocs.at[index_story, 'sentence4']\n",
    "    if 5 in index_rc:\n",
    "        rc += \" \" + rocs.at[index_story, 'sentence5']\n",
    "\n",
    "    # Get masked sentence from sampled story\n",
    "    if index_m == 2:\n",
    "        m = rocs.at[index_story, 'sentence2']\n",
    "    if index_m == 3:\n",
    "        m = rocs.at[index_story, 'sentence3']\n",
    "    if index_m == 4:\n",
    "        m = rocs.at[index_story, 'sentence4']\n",
    "\n",
    "    add_left = random.choice([True, False])\n",
    "    if add_left == True:\n",
    "        false_sample_list.append(lc + m + \" <mask>\" + rc)\n",
    "    else:\n",
    "        false_sample_list.append(lc + \"<mask> \" + m + rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "joint_list = true_sample_list + false_sample_list\n",
    "\n",
    "targets = []\n",
    "for i in range(50000):\n",
    "    targets.append(\"1\")\n",
    "for i in range(50000):\n",
    "    targets.append(\"0\")\n",
    "\n",
    "df_infill = pd.DataFrame()\n",
    "df_infill['text'] = joint_list\n",
    "df_infill['label']  = targets\n",
    "df_infill = df_infill.sample(frac = 1).reset_index(drop=True)   # Shuffle\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "new_dirs_path = os.path.join('data', 'position_classifier') \n",
    "os.makedirs(new_dirs_path, exist_ok=True)              \n",
    "\n",
    "df_infill.to_csv('data/position_classifier/position_classifier.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Build datasets\n",
    "train_test_ratio = 0.9\n",
    "train_valid_ratio = 7/9\n",
    "df_full_train, df_test = train_test_split(df_infill, train_size = train_test_ratio, random_state = 1)\n",
    "df_train, df_valid = train_test_split(df_full_train, train_size = train_valid_ratio, random_state = 1)\n",
    "\n",
    "df_train.to_csv('data/position_classifier/train.csv', index=False)\n",
    "df_valid.to_csv('data/position_classifier/valid.csv', index=False)\n",
    "df_test.to_csv('data/position_classifier/test.csv', index=False)\n",
    "\n",
    "train = load_dataset(\"csv\", data_files='data/position_classifier/train.csv')\n",
    "test = load_dataset(\"csv\", data_files='data/position_classifier/valid.csv')\n",
    "valid = load_dataset(\"csv\", data_files='data/position_classifier/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# tokenized_df = df.map(preprocess_function, batched=True)\n",
    "tokenized_train = train.map(preprocess_function, batched=True)\n",
    "tokenized_test = test.map(preprocess_function, batched=True)\n",
    "tokenized_valid = valid.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='models/position_classifier',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train[\"train\"],\n",
    "    eval_dataset=tokenized_valid[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infill Generator: Generate new infill sentence\n",
    "\n",
    "Input data format:\n",
    "\n",
    "- Training input: \n",
    "\n",
    "    **LC** <INFILL_LOC> **RC** \\<SEP\\> **INFILL** <|endoftext|>\n",
    "\n",
    "- Inference time input: \n",
    "\n",
    "    **LC** <INFILL_LOC> **RC** \\<SEP\\>\n",
    "\n",
    "- Inference time output: \n",
    "\n",
    "    **LC** <INFILL_LOC> **RC** \\<SEP\\> **INFILL** <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "DATA_PATH = 'data/roc_scored.csv'\n",
    "rocs = pd.read_csv(DATA_PATH, usecols=['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5'])\n",
    "\n",
    "story_list = []\n",
    "\n",
    "for i in tqdm(range(len(rocs))):\n",
    "    story = ''\n",
    "\n",
    "    # Choose middle sentence\n",
    "    index_m = random.randint(2,4)\n",
    "    # print(index_m)\n",
    "\n",
    "    # Choose LC and RC\n",
    "    # List of all potential sentences in LC, RC (except endpoints)\n",
    "    potentials_lc = [j for j in range(1,index_m)]\n",
    "    potentials_rc = [j for j in range(index_m+1, 6)]\n",
    "    # Randomly choose LC, RC sentences\n",
    "    index_lc = sorted(random.sample(potentials_lc, random.choice(potentials_lc)))\n",
    "    index_rc = sorted(random.sample(potentials_rc, random.choice(potentials_rc)-index_m))\n",
    "\n",
    "    # Set up LC\n",
    "    story += rocs.at[i, 'sentence1']     # Always have start sentence\n",
    "    if 2 in index_lc:\n",
    "        story += ' ' + rocs.at[i, 'sentence2']\n",
    "    if 3 in index_lc:\n",
    "        story += ' ' + rocs.at[i, 'sentence3']\n",
    "\n",
    "    story += ' <INFILL_LOC> '\n",
    "\n",
    "    # Set up RC\n",
    "    if 3 in index_rc:\n",
    "        story +=  rocs.at[i, 'sentence3'] + ' '\n",
    "    if 4 in index_rc:\n",
    "        story += rocs.at[i, 'sentence4'] + ' '\n",
    "    story += rocs.at[i, 'sentence5']     # Always have stop sentence\n",
    "\n",
    "    # Build target output list\n",
    "    if index_m == 2:\n",
    "        infill_sentence = 'sentence2'\n",
    "    if index_m == 3:\n",
    "        infill_sentence = 'sentence3'\n",
    "    if index_m == 4:\n",
    "        infill_sentence = 'sentence4'\n",
    "    \n",
    "    story += ' <SEP> ' + rocs.at[i, infill_sentence]\n",
    "    story_list.append(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "\n",
    "train_valid_ratio = 0.8\n",
    "df_train, df_valid = train_test_split(story_list, train_size = train_valid_ratio, random_state = 1)\n",
    "\n",
    "def build_dataset(story_list, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    for s in story_list:\n",
    "        s = str(s).strip()\n",
    "        s = re.sub(r\"\\s\", \" \", s)\n",
    "        data += s + '<|endoftext|>\\n'\n",
    "    f.write(data)\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "new_dirs_path = os.path.join('data', 'infill_generator') \n",
    "os.makedirs(new_dirs_path, exist_ok=True) \n",
    "\n",
    "build_dataset(df_train, 'data/infill_generator/train.txt')\n",
    "build_dataset(df_valid, 'data/infill_generator/valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file        data/infill_generator/train.txt \\\n",
    "        --validation_file   data/infill_generator/valid.txt \\\n",
    "        --test_file         data/infill_generator/valid.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path    models/baseline_F1 \\\n",
    "        --output_dir            models/infill_generator \\\n",
    "        --gradient_accumulation_steps 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block multiple times (e.g. 4x) for best results\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file        data/infill_generator/train.txt \\\n",
    "        --validation_file   data/infill_generator/valid.txt \\\n",
    "        --test_file         data/infill_generator/valid.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path    models/infill_generator \\\n",
    "        --output_dir            models/infill_generator \\\n",
    "        --gradient_accumulation_steps 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Models for Ablation Experiments\n",
    "### a2_1: Remove position classifier, infill 3 sentences\n",
    "\n",
    "TODO: Change ablation models here to show ablations in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = 'data/roc_scored.csv'\n",
    "cols = [\"sentence1\", \"sentence2\", \"sentence3\", \"sentence4\", \"sentence5\", \"score\"]\n",
    "rocs = pd.read_csv(data_path, usecols=cols)\n",
    "\n",
    "rocs = rocs[rocs['score'] >= 0.7].reset_index(drop=True)\n",
    "rocs[\"text\"] = rocs[\"sentence1\"] + \" \" + rocs[\"sentence5\"] + \" <MASK> \" + rocs[\"sentence2\"] + \" \" + rocs[\"sentence3\"] + \" \" + rocs[\"sentence4\"] + \"<|endoftext|>\"\n",
    "rocs = rocs.drop(\"sentence1\", axis=1).drop(\"sentence2\", axis=1).drop(\"sentence3\", axis=1).drop(\"sentence4\", axis=1).drop(\"sentence5\", axis=1).drop(\"score\", axis=1)\n",
    "print(f\"Number of samples: {len(rocs)}\")\n",
    "print(f\"Sample: {rocs.at[1, 'text']}\")\n",
    "rocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "def build_dataset(df, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    stories = df['text'].tolist()\n",
    "    for s in stories:\n",
    "        s = str(s).strip()\n",
    "        s = re.sub(r\"\\s\", \" \", s)\n",
    "        data += s + '\\n'\n",
    "    f.write(data)\n",
    "\n",
    "# Build training, validation datasets\n",
    "train_test_ratio = 0.8\n",
    "df_train, df_valid = train_test_split(rocs, train_size = train_test_ratio, random_state = 1)\n",
    "\n",
    "build_dataset(df_train, \"data/train_ablation2_1.txt\")\n",
    "build_dataset(df_valid, \"data/valid_ablation2_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file data/train_ablation2_1.txt \\\n",
    "        --validation_file data/valid_ablation2_1.txt \\\n",
    "        --test_file data/valid_ablation2_1.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path e2e_baseline \\\n",
    "        --output_dir models/ablation2_1 \\\n",
    "        --gradient_accumulation_steps 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a2_1 model\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('models/ablation2_1')\n",
    "model = GPT2LMHeadModel.from_pretrained('models/ablation2_1', pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Sample start, stop\n",
    "starts = [\"Rick grew up in a troubled household. Rick was glad his family was no longer troubled. <MASK>\"]\n",
    "\n",
    "for s in tqdm(starts):\n",
    "    encoded_input = tokenizer.encode(s, return_tensors='pt')\n",
    "    output = model.generate(encoded_input, max_length=256, num_beams=5, early_stopping=True, no_repeat_ngram_size=2)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(decoded_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a2_2: Remove phrase generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate samples (missing sentence)\n",
    "\n",
    "# Each sample is stored as a list of sentences, so during training/inference\n",
    "#   we predict likelihood of sentence infilling between each sentence.\n",
    "# Temporary marker for 1 missing (target) sentence\n",
    "# Temporary markers for alternative missing sentences\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH = './data/roc_scored.csv'\n",
    "cols = [\"sentence1\", \"sentence2\", \"sentence3\", \"sentence4\", \"sentence5\", \"score\"]\n",
    "rocs = pd.read_csv(DATA_PATH, usecols=cols)\n",
    "\n",
    "def gen_samples(num_stories):\n",
    "    all_samples = []\n",
    "    two_s, three_s, four_s = 0, 0, 0    # Count num samples in each len category\\n\",\n",
    "\n",
    "    for index in tqdm(range(num_stories)):\n",
    "\n",
    "        # From S2, S3, S4, pick 0, 1, or 2 sentences\\n\",\n",
    "        #   always want at least 1 infill\\n\",\n",
    "        #   note 3 middle -> 5 total, no infill (neg samples do not include 5)\\n\",\n",
    "        sample_indices = sorted(random.sample([2, 3, 4], random.choice([0, 1, 2])))\n",
    "        # print(\\\"sample indices: {}\\\".format(sample_indices))\\n\",\n",
    "\n",
    "        if len(sample_indices) == 0:\n",
    "            two_s += 1\n",
    "        elif len(sample_indices) == 1:\n",
    "            three_s += 1\n",
    "        elif len(sample_indices) == 2:\n",
    "            four_s += 1\n",
    "\n",
    "        # Generate randomized sample as list of strings (sentences), ints (placeholder masks)\\n\",\n",
    "        sample = []\n",
    "        sample.append(rocs.at[index, 'sentence1'])        # start always included\\n\",\n",
    "        missing = []\n",
    "\n",
    "        if 2 in sample_indices:\n",
    "            sample.append(rocs.at[index, 'sentence2'])    \n",
    "        else:\n",
    "            missing.append(rocs.at[index, 'sentence2']) \n",
    "\n",
    "        if 3 in sample_indices:\n",
    "            sample.append(rocs.at[index, 'sentence3'])\n",
    "        else:\n",
    "            missing.append(rocs.at[index, 'sentence3']) \n",
    "\n",
    "        if 4 in sample_indices:\n",
    "            sample.append(rocs.at[index, 'sentence4'])\n",
    "        else:\n",
    "            missing.append(rocs.at[index, 'sentence4']) \n",
    "\n",
    "        infill = choice(missing)\n",
    "        sample.append(rocs.at[index, 'sentence5'])        # stop always included\n",
    "        sample = ' '.join(sample)\n",
    "        sample += ' <SEP> ' + infill + '<|endoftext|>'\n",
    "        \n",
    "        all_samples.append(sample)\n",
    "\n",
    "        \n",
    "\n",
    "    print('-'*40)\n",
    "    print('samples with 2 sentences: {}'.format(two_s))\n",
    "    print('samples with 3 sentences: {}'.format(three_s))\n",
    "    print('samples with 4 sentences: {}'.format(four_s))\n",
    "    print('-'*40)\n",
    "    return all_samples\n",
    "\n",
    "samples = gen_samples(len(rocs))\n",
    "df = pd.DataFrame()\n",
    "df['text'] = samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "def build_dataset(df, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    stories = df['text'].tolist()\n",
    "    for s in stories:\n",
    "        s = str(s).strip()\n",
    "        s = re.sub(r\"\\s\", \" \", s)\n",
    "        data += s + '\\n'\n",
    "    f.write(data)\n",
    "\n",
    "# Build training, validation datasets\n",
    "train_test_ratio = 0.8\n",
    "df_train, df_valid = train_test_split(df, train_size = train_test_ratio, random_state = 1)\n",
    "\n",
    "build_dataset(df_train, \"data/train_ablation2_3.txt\")\n",
    "build_dataset(df_valid, \"data/valid_ablation2_3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 run_clm_no_trainer.py \\\n",
    "        --train_file data/train_ablation2_3.txt \\\n",
    "        --validation_file data/valid_ablation2_3.txt \\\n",
    "        --test_file data/valid_ablation2_3.txt \\\n",
    "        --do_train \\\n",
    "        --model_name_or_path e2e_baseline \\\n",
    "        --output_dir models/ablation2_3 \\\n",
    "        --gradient_accumulation_steps 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
